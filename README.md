# NNumpy: Basic Neural Network Architecture in Pure NumPy

This repository provides a modular, fundamental implementation of a Neural Network architecture using only NumPy.

Modern Deep Neural Networks (DNNs) are powerful because they include a ton of advanced techniques and optimizations. However, this implementation skips most of that to keep things simple. The idea is to help you see how neural networks function at a basic level—like forward propagation, backpropagation, and gradient descent—without getting bogged down by the complexities.

## Key Things to Know

- This project is meant to be educational. It’s not built for production use or handling big, complex tasks. Instead, it focuses on helping you understand the core concepts in a straightforward way.

- The code is designed to be modular, so you can easily tinker with different parts—like adding new activation functions, layers, or playing around with different optimization strategies.

- By sticking to just NumPy, the code stays simple and easy to follow. This helps you get a clear view of the math and algorithms behind neural networks without extra layers of abstraction.

## Whats Missing?

Since this project is meant for learning, it doesn’t include a lot of the advanced stuff you'd find in other libraries like TensorFlow or PyTorch. Here’s what you won’t find here:

- Regularization methods like dropout or L2 regularization
- Fancy optimizers like Adam or RMSprop
- Advanced neural network layers like convolutional or recurrent layers
- Support for large-scale datasets or complex models
- GPU support or parallel processing